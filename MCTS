import chess
import chess.pgn
import numpy as np
import tensorflow as tf
from collections import defaultdict
import chess.svg

class MCTSNode:
    def __init__(self, parent, move):
        self.parent = parent
        self.move = move
        self.children = []
        self.visits = 0
        self.score = 0

def create_policy_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=(8, 8, 12)),
        tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),
        tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(4672, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='categorical_crossentropy')
    return model

def board_to_input(board):
    input_ = np.zeros((8, 8, 12), dtype=np.float32)
    for square, piece in board.piece_map().items():
        x = square % 8
        y = square // 8
        piece_idx = piece.piece_type - 1 + (6 if piece.color == chess.BLACK else 0)
        input_[y, x, piece_idx] = 1.0
    return input_

def mcts_search(node, board, model, simulations=800):
    for _ in range(simulations):
        child_node, child_board = select_child(node, board, model)
        reward = simulate(child_board)
        backpropagate(child_node, reward)

def select_child(node, board, model):
    unexplored_moves = [move for move in board.legal_moves if move not in [child.move for child in node.children]]
    if unexplored_moves:
        move = np.random.choice(unexplored_moves)
        child_node = MCTSNode(node, move)
        node.children.append(child_node)
        board.push(move)
        return child_node, board

    policy = model.predict(np.array([board_to_input(board)]))[0]
    legal_policy = [policy[chess.Move.from_uci(uci).uci()] for uci in board.legal_moves]
    ucb_values = [((child.score / child.visits) + np.sqrt(np.log(node.visits) / child.visits)) * policy_value
                  for child, policy_value in zip(node.children, legal_policy)]
    child_node = node.children[np.argmax(ucb_values)]
    board.push(child_node.move)
    return child_node, board

def simulate(board):
    while not board.is_game_over():
        board.push(np.random.choice(list(board.legal_moves)))
    return board.result()

def backpropagate(node, reward):
    node.visits += 1
    node.score += reward
    if node.parent is not None:
        backpropagate(node.parent, -reward)

def self_play(model, simulations=800):
    board = chess.Board()
    root_node = MCTSNode(None, None)
    move_history = []

    while not board.is_game_over():
        mcts_search(root_node, board, model, simulations)
        best_child = max(root_node.children, key=lambda child: child.visits)
        move_history.append((board_to_input(board), best_child.move))

        # Display the chess board
        display_board(board, move_history)

        board.push(best_child.move)
        root_node = best_child
        root_node.parent = None

    return move_history, board.result()

def display_board(board, move_history):
    last_move = move_history[-1][1] if move_history else None
    board_svg = chess.svg.board(board=board, lastmove=last_move, size=400)
    print(board_svg)

def main():
    model = create_policy_model()
    checkpoint_path = "model_checkpoints/cp.ckpt"
    model.load_weights(checkpoint_path)

    move_history, result = self_play(model, simulations=800)
    print("Game Result:", result)

    # You can do something with the move_history here, like training the model further or analyzing the game.

if __name__ == "__main__":
    main()
